{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elimeleth/aprende_a_crear_asistentes/blob/main/langchain_ragas_gro0q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPglrYdWqkcs"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain==0.2.7 tantivy lancedb langchain-groq langchain-google-genai sentence_transformers==3.0.1 langchain-community==0.2.7 langchain_experimental==0.0.62 torch==2.3.0 ragas==0.1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SB2rns9iduG"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Some async tasks need to be done\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlxkvHhfr5CI"
      },
      "outputs": [],
      "source": [
        "# from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import time\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "critic_llm = ChatGoogleGenerativeAI(temperature=0,model='gemini-1.5-flash', api_key =userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7vUPuUlrn7M"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LHKCxXzkLSO"
      },
      "outputs": [],
      "source": [
        "def trace_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        inicio = time.time()\n",
        "        resultado = func(*args, **kwargs)\n",
        "        fin = time.time()\n",
        "\n",
        "        print(\"#\"*20, \"Tiempo de ejecucion\", \"#\"*20)\n",
        "        print(f\"La función {func.__name__} tardó {fin - inicio:.4f} segundos en ejecutarse.\")\n",
        "        print(\"#\"*50)\n",
        "\n",
        "        return resultado\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L15-HY6CgtYu"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "@trace_time\n",
        "def talk_to_assistant(input, prompt=\"Eres un asistente util\"):\n",
        "  return ChatPromptTemplate.from_template(prompt).pipe(llm).invoke({\"input\": input})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iTl2_pOi9Go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebafd129-cca9-4765-d08a-eb75a034e3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################### Tiempo de ejecucion ####################\n",
            "La función talk_to_assistant tardó 0.5699 segundos en ejecutarse.\n",
            "##################################################\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='¡Gracias! Me esfuerzo por ser lo más útil posible. ¿En qué puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 42, 'total_tokens': 68, 'completion_time': 0.104, 'prompt_time': 0.009984519, 'queue_time': 0.19843529000000001, 'total_time': 0.113984519}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-28519759-deef-4e5a-9e50-d79bc01fda10-0', usage_metadata={'input_tokens': 42, 'output_tokens': 26, 'total_tokens': 68})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "talk_to_assistant(\"hola\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c-PyboWsxXk"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# Load Data\n",
        "import random\n",
        "\n",
        "@trace_time\n",
        "def loader_text_splitter(chunk_size, chunk_overlap):\n",
        "  loader = TextLoader('./data.txt')\n",
        "  documents = loader.load()\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  return text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "\n",
        "@trace_time\n",
        "def loader_semantic_splitter(type):\n",
        "  loader = TextLoader('./data.txt')\n",
        "  documents = loader.load()\n",
        "  text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=type)\n",
        "  return text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMD_EBQjt-0H"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import LanceDB\n",
        "\n",
        "@trace_time\n",
        "def upsert_docs(docs):\n",
        "  vector_store = LanceDB(\n",
        "      table_name=\"test\",\n",
        "      embedding=embeddings,\n",
        "  )\n",
        "  vector_store.add_documents(docs)\n",
        "  return vector_store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMjYaRpzvTzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9daf2be-baaa-4ac7-e8f4-776cbf932809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1197, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1450, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1012, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1353, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################### Tiempo de ejecucion ####################\n",
            "La función loader_text_splitter tardó 0.0088 segundos en ejecutarse.\n",
            "##################################################\n",
            "#################### Tiempo de ejecucion ####################\n",
            "La función upsert_docs tardó 2.3003 segundos en ejecutarse.\n",
            "##################################################\n"
          ]
        }
      ],
      "source": [
        "vector_store = upsert_docs(loader_text_splitter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTq22BDXvW62"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
        "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
        "\n",
        "\n",
        "{context}\n",
        "\n",
        "\n",
        "QUESTION:```{question}```\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
        "  )\n",
        "#\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "@trace_time\n",
        "def return_chain(llm, vector_store):\n",
        "  retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "  return RetrievalQA.from_chain_type(llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": PROMPT},\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True)"
      ],
      "metadata": {
        "id": "sNrQ3RLF_WdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm= llm,\n",
        "    critic_llm=critic_llm,\n",
        "    embeddings=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "G2uFtSA1-AHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset = generator.generate_with_langchain_docs(\n",
        "    loader_text_splitter(),\n",
        "    test_size=5,\n",
        "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
        ")"
      ],
      "metadata": {
        "id": "VEQw3MZM-EYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = testset.to_pandas()\n",
        "len(df)"
      ],
      "metadata": {
        "id": "RjK1_Bce-FqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9866bb98-5938-4228-e37c-61a134c8dcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "haiDkTthCtf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3458f6-30a8-490d-eda7-6e417219ce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def create_eval_dataset(dataset, docs, eval_size,retrieval_window_size):\n",
        "    vs = upsert_docs(docs)\n",
        "    chain = return_chain(llm, vs)\n",
        "    questions = []\n",
        "    answers = []\n",
        "    contexts = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for i in range(eval_size):\n",
        "        print(\"eval\", i)\n",
        "        entry = dataset.iloc[i]\n",
        "        question = entry['question']\n",
        "        ground_truth = entry['ground_truth']\n",
        "        questions.append(question)\n",
        "        ground_truths.append(ground_truth)\n",
        "        response = chain(question)\n",
        "        answer = response['result']\n",
        "        context = [doc.page_content for doc in response['source_documents'][:retrieval_window_size]]\n",
        "        contexts.append(context)\n",
        "        answers.append(answer)\n",
        "\n",
        "    rag_response_data = {\n",
        "        \"question\": questions,\n",
        "        \"answer\": answers,\n",
        "        \"contexts\": contexts,\n",
        "        \"ground_truth\": ground_truths\n",
        "    }\n",
        "\n",
        "    return rag_response_data\n"
      ],
      "metadata": {
        "id": "ViGpyRLq-JRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "def evaluate_response_time_and_accuracy(docs, ds_dict, llm, embed_model):\n",
        "    dataset = Dataset.from_dict(ds_dict)\n",
        "\n",
        "    metrics = [\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Evaluate using Ragas\n",
        "    start_time = time.time()\n",
        "    result = evaluate(\n",
        "        metrics=metrics,\n",
        "        dataset=dataset,\n",
        "        llm=llm,\n",
        "        embeddings=embed_model,\n",
        "        raise_exceptions=False,\n",
        "    )\n",
        "    average_response_time = time.time() - start_time\n",
        "    average_faithfulness = result['faithfulness']\n",
        "    average_answer_relevancy = result['answer_relevancy']\n",
        "    average_context_precision = result['context_precision']\n",
        "    average_context_recall = result['context_recall']\n",
        "\n",
        "    return (average_response_time, average_faithfulness, average_answer_relevancy,\n",
        "            average_context_precision, average_context_recall)"
      ],
      "metadata": {
        "id": "SXyOwY53-XY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 45\n",
        "EVAL_SIZE = len(df)\n",
        "RETRIEVAL_WINDOW_SIZE = 2"
      ],
      "metadata": {
        "id": "AxV-iYS-LXP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chunk_size_chunk_overlap(docs, ds_dict):\n",
        "  (avg_time, avg_faithfulness, avg_answer_relevancy,avg_context_precision, avg_context_recall) = evaluate_response_time_and_accuracy(docs, ds_dict, llm, embeddings)\n",
        "  print(f\"Chunk size {CHUNK_SIZE}, Overlap {chunk_overlap} - \"\n",
        "        f\"Average Response time: {avg_time:.2f}s, \"\n",
        "        f\"Average Faithfulness: {avg_faithfulness:.2f}, \"\n",
        "        f\"Average Answer Relevancy: {avg_answer_relevancy:.2f}, \"\n",
        "        f\"Average Context Precision: {avg_context_precision:.2f}, \"\n",
        "        f\"Average Context Recall: {avg_context_recall:.2f}\")\n",
        "\n",
        "def evaluate_semantic_chunker(docs, ds_dict):\n",
        "  (avg_time, avg_faithfulness, avg_answer_relevancy,avg_context_precision, avg_context_recall) = evaluate_response_time_and_accuracy(docs, ds_dict, llm, embeddings)\n",
        "  print(f\"Chunk size {CHUNK_SIZE}, Overlap {chunk_overlap} - \"\n",
        "      f\"Average Response time: {avg_time:.2f}s, \"\n",
        "      f\"Average Faithfulness: {avg_faithfulness:.2f}, \"\n",
        "      f\"Average Answer Relevancy: {avg_answer_relevancy:.2f}, \"\n",
        "      f\"Average Context Precision: {avg_context_precision:.2f}, \"\n",
        "      f\"Average Context Recall: {avg_context_recall:.2f}\")"
      ],
      "metadata": {
        "id": "iugovzk0DIcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader_text_splitter(40, 15)\n",
        "ds_dict= create_eval_dataset(df,docs,EVAL_SIZE,RETRIEVAL_WINDOW_SIZE)"
      ],
      "metadata": {
        "id": "hhV9u3VfFD50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_chunk_size_chunk_overlap(docs, ds_dict)"
      ],
      "metadata": {
        "id": "3agZPTD3ED-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk_overlap in range(0,CHUNK_SIZE,15):\n",
        "  docs = loader_text_splitter(CHUNK_SIZE, chunk_overlap)\n",
        "  ds_dict= create_eval_dataset(df,docs,EVAL_SIZE,RETRIEVAL_WINDOW_SIZE)\n",
        "  evaluate_chunk_size_chunk_overlap(docs, ds_dict)\n",
        "  time.sleep(2)"
      ],
      "metadata": {
        "id": "lquX_jI7_01L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_semantic_chunker(\"gradient\")"
      ],
      "metadata": {
        "id": "haoCnNwdEHAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in [\"percentile\",\"standard_deviation\",\"interquartile\",\"gradient\"]:\n",
        "  docs = loader_semantic_splitter(t)\n",
        "  ds_dict= create_eval_dataset(df,docs,EVAL_SIZE,RETRIEVAL_WINDOW_SIZE)\n",
        "  evaluate_semantic_chunker(docs, ds_dict)\n",
        "  time.sleep(2)"
      ],
      "metadata": {
        "id": "oHDEDmtc_E7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PbEb0jNQBRlx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObRb87Q1Q9kCgDSyN84b8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}